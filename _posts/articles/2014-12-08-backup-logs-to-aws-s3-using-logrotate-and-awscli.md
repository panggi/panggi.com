---
layout: post
title: "Backup Logs to AWS S3 Using Logrotate and Awscli"
modified:
categories: articles
excerpt: "As developer, logging feature is super useful for debugging and in production it is also can be used for tracking users and security mitigation as well."
tags: [cloud, python]
image:
  feature: featured/cloud.jpg
  credit: datatrend.com
  creditlink: http://www.datatrend.com/optimize-it/wp-content/uploads/2012/08/private-cloud2.jpg
date: 2014-12-08T11:02:18+07:00
comments: true
---

## Motivation

As developer, logging feature is super useful for debugging and in production it is also can be used for tracking users and security mitigation as well.

But the problem comes when the size of log is increasing and make your disk full that will affect the performance of your web applications.

In this article i will share the tips using two apps:

* **logrotate**: designed to ease administration of systems that generate large numbers of log files. It allows automatic rotation, compression, removal, and mailing of log files.
* **awscli**: Universal Command Line Interface for Amazon Web Services.

## Installation

**awscli**

Prerequisites: Python 2.6.3 or later

    {% raw %}
    $ curl "https://bootstrap.pypa.io/get-pip.py" -o "get-pip.py"
    $ sudo python get-pip.py
    $ sudo pip install awscli
    {% endraw %}

**logrotate**

logrotate is preinstalled with major GNU/Linux distribution, so i assume your production server might have one installed already.

## Configuration

First thing first, lets make sure that our awscli can upload files to AWS S3.

    {% raw %}
    $ aws configure
    AWS Access Key ID [None]: AKIAIOSFODNN7EXAMPLE
    AWS Secret Access Key [None]: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY
    Default region name [None]: ap-southeast-1
    Default output format [None]: json
    {% endraw %}

Lets try to upload some files recursively using **cp** [command](http://docs.aws.amazon.com/cli/latest/reference/):

    $ aws s3 cp myfolder s3://mybucket/myfolder --recursive 
    upload: myfolder/file1.txt to s3://mybucket/myfolder/file1.txt 
    upload: myfolder/subfolder/file1.txt to s3://mybucket/myfolder/subfolder/file1.txt 
    ...

Ok, it works, now let's continue with logrotate. There's a nice tutorial by [Rackspace](http://www.rackspace.com/knowledge_center/article/understanding-logrotate-utility) and i recommend you to see it.

I'm going to use **nginx** logrotate sample generated by nginx during installation with package manager:

    {% raw %}
    $ vim /etc/logrotate.d/nginx

    /var/log/nginx/*log {
        create 0644 nginx nginx
        daily
        rotate 1
        missingok
        notifempty
        compress
        sharedscripts
        postrotate
            /bin/kill -USR1 `cat /run/nginx.pid 2>/dev/null` 2>/dev/null || true
        endscript
    }
    {% endraw %}

So, this configuration will run daily, will keep 1 log and remove the older one, compress the nonactive logs and will kill nginx after log rotation done.

## Conclusion

So you got the idea why it is great to combine logrotate and awscli, you will save disk space at your production server and your AWS S3 volumes by not uploading the same log files again and again.

## Tip

If your logrotate is not running, you can check the status at **cat /var/lib/logrotate.status** for centos or **/var/lib/logrotate/status** for ubuntu

